{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.io.image import read_image, ImageReadMode\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(image_dir, num_triplets):\n",
    "    dirnames = os.listdir(image_dir)\n",
    "    triplets = set()\n",
    "\n",
    "    while len(triplets) < num_triplets:\n",
    "        # Randomly sample an anchor image\n",
    "        anchor_dirname = random.choice(dirnames)\n",
    "        anchor_filename = random.choice(os.listdir(os.path.join(image_dir, anchor_dirname)))\n",
    "        anchor_path = os.path.join(image_dir, anchor_dirname, anchor_filename)\n",
    "\n",
    "        # Randomly sample a positive image (same class as anchor)\n",
    "        positive_dirname = anchor_dirname\n",
    "        positive_dir_files = os.listdir(os.path.join(image_dir, positive_dirname))\n",
    "        positive_dir_files.remove(anchor_filename)\n",
    "        positive_filename = random.choice(positive_dir_files)\n",
    "        positive_path = os.path.join(image_dir, positive_dirname, positive_filename)\n",
    "\n",
    "        # Randomly sample a negative image (different class from anchor)\n",
    "        dirnames_ = dirnames.copy()\n",
    "        dirnames_.remove(anchor_dirname)\n",
    "        negative_dirname = random.choice(dirnames_)\n",
    "        negative_filename = random.choice(os.listdir(os.path.join(image_dir, negative_dirname)))\n",
    "        negative_path = os.path.join(image_dir, negative_dirname, negative_filename)\n",
    "\n",
    "        triplets.add((anchor_path, positive_path, negative_path))\n",
    "\n",
    "    triplets = list(triplets)\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTripletDataset(Dataset):\n",
    "    def __init__(self, triplets, transform=None):\n",
    "        self.triplets = triplets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor_path, positive_path, negative_path = self.triplets[index]\n",
    "\n",
    "        anchor_image = read_image(anchor_path, mode=ImageReadMode.RGB) / 255.0\n",
    "        positive_image = read_image(positive_path, mode=ImageReadMode.RGB) / 255.0\n",
    "        negative_image = read_image(negative_path, mode=ImageReadMode.RGB) / 255.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            anchor_image = self.transform(anchor_image)\n",
    "            positive_image = self.transform(positive_image)\n",
    "            negative_image = self.transform(negative_image)\n",
    "\n",
    "        return anchor_image, positive_image, negative_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image directory and other parameters\n",
    "image_dir = \"dataset/CUB_200_2011/images\"\n",
    "num_triplets = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Sample image triplets\n",
    "triplets = sample_triplets(image_dir, num_triplets)\n",
    "\n",
    "# Define the data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create the image triplet dataset\n",
    "dataset = ImageTripletDataset(triplets, transform=transform)\n",
    "\n",
    "# Create the data loader\n",
    "dataloader = DataLoader(dataset, batch_size=None, shuffle=True)\n",
    "\n",
    "# # Iterate through the data loader\n",
    "# for i, (anchor, positive, negative) in enumerate(dataloader):\n",
    "#     # Process the anchor, positive, and negative images\n",
    "#     # Perform any necessary operations or training steps\n",
    "#     if i == 0:\n",
    "#         print(anchor.shape)\n",
    "#         print(positive.shape)\n",
    "#         print(negative.shape)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Define your dataset and data loaders for anchor, positive, and negative examples\n",
    "\n",
    "# Define the triplet loss function\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = torch.dist(anchor, positive, p=2)\n",
    "        distance_negative = torch.dist(anchor, negative, p=2)\n",
    "        loss = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained ResNet18 model\n",
    "pretrained_model = resnet18(pretrained=True)\n",
    "\n",
    "# Create the finetuning model with the pretrained backbone\n",
    "finetuned_model = nn.Sequential(*list(pretrained_model.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze the model parameters\n",
    "# for name, param in finetuned_model.named_parameters():\n",
    "#     if name == '6' or name == '7' or name == '8':\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.9815043807029724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:01<00:13,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 1.0720288634300232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:02<00:12,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 0.5543064594268798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:02<00:11,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 0.42964686155319215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:03<00:10,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.7255280315876007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:04<00:10,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 0.3984716713428497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:05<00:09,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 0.1897673487663269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:05<00:08,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 0.0863064169883728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:06<00:07,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 0.21846996545791625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:07<00:07,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 0.4011864960193634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:07<00:06,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 0.04901754856109619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:08<00:05,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:09<00:05,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:10<00:04,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:10<00:03,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:11<00:02,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:12<00:02,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:12<00:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:13<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:14<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the triplet loss\n",
    "triplet_loss = TripletLoss()\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(finetuned_model.parameters(), lr=0.001)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "finetuned_model.to(device)\n",
    "finetuned_model.train()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for anchor, positive, negative in dataloader:\n",
    "        anchor = anchor.to(device)              # [3, 224, 224]\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embedding = finetuned_model(anchor.unsqueeze(0)).flatten()       # [512, ]\n",
    "        positive_embedding = finetuned_model(positive.unsqueeze(0)).flatten()\n",
    "        negative_embedding = finetuned_model(negative.unsqueeze(0)).flatten()\n",
    "\n",
    "        loss = triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
    "        '''\n",
    "            # In case the previous cell to freeze model params is run, \n",
    "            the next line must be uncommented to solve the error : \n",
    "            RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\n",
    "            TODO: \n",
    "            On freezing model params for layers 6, 7 (both blocks of conv, batchnorm layers), 8 (Avg Pool), \n",
    "                the loss doesn't decrease at all!\n",
    "        '''\n",
    "        # loss.requires_grad = True #### (Only when freezing some model params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Evaluate or save the finetuned model\n",
    "model_path = \"../models/resnet18_triplet_finetuned.pth\"\n",
    "torch.save(finetuned_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/resnet18_triplet_finetuned.pth\"\n",
    "finetuned_model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class dictionary to store mappings of class names with class ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prothonotary_warbler': '177',\n",
       " 'yellow_throated_vireo': '157',\n",
       " 'prairie_warbler': '176',\n",
       " 'cardinal': '017',\n",
       " 'sooty_albatross': '003',\n",
       " 'florida_jay': '074',\n",
       " 'olive_sided_flycatcher': '040',\n",
       " 'swainson_warbler': '178',\n",
       " 'cape_glossy_starling': '134',\n",
       " 'chestnut_sided_warbler': '165',\n",
       " 'yellow_bellied_flycatcher': '043',\n",
       " 'northern_waterthrush': '183',\n",
       " 'ruby_throated_hummingbird': '068',\n",
       " 'ringed_kingfisher': '082',\n",
       " 'great_grey_shrike': '112',\n",
       " 'parakeet_auklet': '007',\n",
       " 'red_winged_blackbird': '010',\n",
       " 'geococcyx': '110',\n",
       " 'pine_warbler': '175',\n",
       " 'white_eyed_vireo': '156',\n",
       " 'field_sparrow': '119',\n",
       " 'tropical_kingbird': '077',\n",
       " 'nelson_sharp_tailed_sparrow': '126',\n",
       " 'western_grebe': '053',\n",
       " 'bewick_wren': '193',\n",
       " 'mockingbird': '091',\n",
       " 'brewer_sparrow': '115',\n",
       " 'ring_billed_gull': '064',\n",
       " 'house_sparrow': '118',\n",
       " 'cedar_waxwing': '186',\n",
       " 'indigo_bunting': '014',\n",
       " 'herring_gull': '062',\n",
       " 'grasshopper_sparrow': '121',\n",
       " 'vesper_sparrow': '131',\n",
       " 'seaside_sparrow': '128',\n",
       " 'heermann_gull': '061',\n",
       " 'philadelphia_vireo': '153',\n",
       " 'bay_breasted_warbler': '158',\n",
       " 'chuck_will_widow': '022',\n",
       " 'western_gull': '066',\n",
       " 'white_necked_raven': '108',\n",
       " 'spotted_catbird': '018',\n",
       " 'red_headed_woodpecker': '191',\n",
       " 'yellow_headed_blackbird': '012',\n",
       " 'gray_kingbird': '078',\n",
       " 'savannah_sparrow': '127',\n",
       " 'least_tern': '147',\n",
       " 'clay_colored_sparrow': '117',\n",
       " 'nashville_warbler': '172',\n",
       " 'wilson_warbler': '180',\n",
       " 'loggerhead_shrike': '111',\n",
       " 'fish_crow': '030',\n",
       " 'horned_lark': '085',\n",
       " 'northern_flicker': '036',\n",
       " 'boat_tailed_grackle': '049',\n",
       " 'black_tern': '142',\n",
       " 'house_wren': '196',\n",
       " 'acadian_flycatcher': '037',\n",
       " 'artic_tern': '141',\n",
       " 'sage_thrasher': '150',\n",
       " 'red_cockaded_woodpecker': '190',\n",
       " 'black_billed_cuckoo': '031',\n",
       " 'canada_warbler': '162',\n",
       " 'green_jay': '075',\n",
       " 'black_throated_sparrow': '114',\n",
       " 'great_crested_flycatcher': '038',\n",
       " 'frigatebird': '044',\n",
       " 'cliff_swallow': '137',\n",
       " 'yellow_billed_cuckoo': '033',\n",
       " 'palm_warbler': '174',\n",
       " 'summer_tanager': '140',\n",
       " 'painted_bunting': '016',\n",
       " 'white_crowned_sparrow': '132',\n",
       " 'blue_grosbeak': '054',\n",
       " 'blue_jay': '073',\n",
       " 'baltimore_oriole': '095',\n",
       " 'myrtle_warbler': '171',\n",
       " 'bobolink': '013',\n",
       " 'gadwall': '046',\n",
       " 'brown_creeper': '028',\n",
       " 'lincoln_sparrow': '125',\n",
       " 'groove_billed_ani': '004',\n",
       " 'brown_pelican': '100',\n",
       " 'eastern_towhee': '021',\n",
       " 'evening_grosbeak': '055',\n",
       " 'pied_kingfisher': '081',\n",
       " 'pine_grosbeak': '056',\n",
       " 'pied_billed_grebe': '052',\n",
       " 'mallard': '087',\n",
       " 'california_gull': '059',\n",
       " 'pigeon_guillemot': '058',\n",
       " 'red_eyed_vireo': '154',\n",
       " 'cerulean_warbler': '164',\n",
       " 'long_tailed_jaeger': '071',\n",
       " 'eared_grebe': '050',\n",
       " 'harris_sparrow': '122',\n",
       " 'white_breasted_kingfisher': '083',\n",
       " 'mangrove_cuckoo': '032',\n",
       " 'american_three_toed_woodpecker': '187',\n",
       " 'least_auklet': '006',\n",
       " 'tree_swallow': '138',\n",
       " 'horned_puffin': '106',\n",
       " 'blue_winged_warbler': '161',\n",
       " 'hooded_warbler': '167',\n",
       " 'american_redstart': '109',\n",
       " 'marsh_wren': '197',\n",
       " 'pileated_woodpecker': '188',\n",
       " 'forsters_tern': '146',\n",
       " 'red_faced_cormorant': '024',\n",
       " 'purple_finch': '035',\n",
       " 'cactus_wren': '194',\n",
       " 'yellow_breasted_chat': '020',\n",
       " 'red_legged_kittiwake': '084',\n",
       " 'carolina_wren': '195',\n",
       " 'orchard_oriole': '097',\n",
       " 'brewer_blackbird': '009',\n",
       " 'black_throated_blue_warbler': '160',\n",
       " 'yellow_warbler': '182',\n",
       " 'barn_swallow': '136',\n",
       " 'brandt_cormorant': '023',\n",
       " 'song_sparrow': '129',\n",
       " 'worm_eating_warbler': '181',\n",
       " 'shiny_cowbird': '027',\n",
       " 'black_capped_vireo': '151',\n",
       " 'western_meadowlark': '088',\n",
       " 'fox_sparrow': '120',\n",
       " 'elegant_tern': '145',\n",
       " 'rusty_blackbird': '011',\n",
       " 'vermilion_flycatcher': '042',\n",
       " 'black_and_white_warbler': '159',\n",
       " 'brown_thrasher': '149',\n",
       " 'tree_sparrow': '130',\n",
       " 'scarlet_tanager': '139',\n",
       " 'winter_wren': '199',\n",
       " 'crested_auklet': '005',\n",
       " 'bronzed_cowbird': '026',\n",
       " 'scott_oriole': '098',\n",
       " 'common_tern': '144',\n",
       " 'common_raven': '107',\n",
       " 'european_goldfinch': '048',\n",
       " 'baird_sparrow': '113',\n",
       " 'white_breasted_nuthatch': '094',\n",
       " 'rhinoceros_auklet': '008',\n",
       " 'pelagic_cormorant': '025',\n",
       " 'northern_fulmar': '045',\n",
       " 'downy_woodpecker': '192',\n",
       " 'sayornis': '103',\n",
       " 'caspian_tern': '143',\n",
       " 'american_pipit': '104',\n",
       " 'white_throated_sparrow': '133',\n",
       " 'orange_crowned_warbler': '173',\n",
       " 'ovenbird': '099',\n",
       " 'rufous_hummingbird': '069',\n",
       " 'nighthawk': '092',\n",
       " 'louisiana_waterthrush': '184',\n",
       " 'red_breasted_merganser': '090',\n",
       " 'american_crow': '029',\n",
       " 'least_flycatcher': '039',\n",
       " 'tennessee_warbler': '179',\n",
       " 'le_conte_sparrow': '124',\n",
       " 'pacific_loon': '086',\n",
       " 'lazuli_bunting': '015',\n",
       " 'belted_kingfisher': '079',\n",
       " 'anna_hummingbird': '067',\n",
       " 'glaucous_winged_gull': '060',\n",
       " 'common_yellowthroat': '200',\n",
       " 'chipping_sparrow': '116',\n",
       " 'pomarine_jaeger': '072',\n",
       " 'gray_crowned_rosy_finch': '034',\n",
       " 'red_bellied_woodpecker': '189',\n",
       " 'gray_catbird': '019',\n",
       " 'horned_grebe': '051',\n",
       " 'henslow_sparrow': '123',\n",
       " 'ivory_gull': '063',\n",
       " 'bank_swallow': '135',\n",
       " 'clark_nutcracker': '093',\n",
       " 'golden_winged_warbler': '166',\n",
       " 'green_violetear': '070',\n",
       " 'laysan_albatross': '002',\n",
       " 'whip_poor_will': '105',\n",
       " 'hooded_oriole': '096',\n",
       " 'blue_headed_vireo': '152',\n",
       " 'dark_eyed_junco': '076',\n",
       " 'scissor_tailed_flycatcher': '041',\n",
       " 'kentucky_warbler': '168',\n",
       " 'rose_breasted_grosbeak': '057',\n",
       " 'green_kingfisher': '080',\n",
       " 'hooded_merganser': '089',\n",
       " 'black_footed_albatross': '001',\n",
       " 'rock_wren': '198',\n",
       " 'bohemian_waxwing': '185',\n",
       " 'western_wood_pewee': '102',\n",
       " 'slaty_backed_gull': '065',\n",
       " 'magnolia_warbler': '169',\n",
       " 'warbling_vireo': '155',\n",
       " 'cape_may_warbler': '163',\n",
       " 'white_pelican': '101',\n",
       " 'green_tailed_towhee': '148',\n",
       " 'mourning_warbler': '170',\n",
       " 'american_goldfinch': '047'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = \"dataset/CUB_200_2011/images/\"\n",
    "dir_list = os.listdir(img_path)\n",
    "class_dict = {}\n",
    "for dirname in dir_list:\n",
    "    tokens = dirname.split(\".\")\n",
    "    class_dict[tokens[1].lower()] = tokens[0]\n",
    "\n",
    "class_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute embeddings for query and gallery images using the finetuned `resnet18` encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import normalize, resize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute the embeddings for each image in a input path using model defined above and save them in a output path\n",
    "def compute_and_save_embeddings(inp_path : str, out_path : str):\n",
    "    # create output directories\n",
    "    os.makedirs(out_path + \"/query\", exist_ok=True)\n",
    "    os.makedirs(out_path + \"/gallery\", exist_ok=True)\n",
    "\n",
    "    query_path = inp_path + \"/query\"\n",
    "    gallery_path = inp_path + \"/gallery\"\n",
    "\n",
    "    # compute embeddings for query images\n",
    "    query_files = os.listdir(query_path)\n",
    "    for file in query_files:\n",
    "        img = read_image(query_path + \"/\" + file)\n",
    "        input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        out = finetuned_model(input_tensor.unsqueeze(0))\n",
    "        output_tensor = out.flatten()\n",
    "        torch.save(output_tensor, out_path + \"/query/\" + file[:-4] + \".pt\")\n",
    "\n",
    "    # compute embeddings for gallery images\n",
    "    gallery_dirnames = os.listdir(gallery_path)\n",
    "    for dirname in gallery_dirnames:\n",
    "        os.makedirs(out_path + \"/gallery\" + \"/\" + dirname, exist_ok=True)\n",
    "        gallery_files = os.listdir(gallery_path + \"/\" + dirname)\n",
    "        \n",
    "        for file in gallery_files:\n",
    "            img = read_image(gallery_path + \"/\" + dirname + \"/\" + file)\n",
    "            input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            out = finetuned_model(input_tensor.unsqueeze(0))\n",
    "            output_tensor = out.flatten() \n",
    "            torch.save(output_tensor, out_path + \"/gallery/\" + dirname + \"/\" + file[:-4] + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyas/anaconda3/envs/ets/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/shreyas/Desktop/Project/data/Stylianou_resnet18_finetuned.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/shreyas/Desktop/Project/data/Stylianou_resnet18_finetuned.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m compute_and_save_embeddings(\u001b[39m\"\u001b[39;49m\u001b[39mdataset/img_retrieval_CUB_200_2011\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39membeddings_CUB_200_2011_ft\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/home/shreyas/Desktop/Project/data/Stylianou_resnet18_finetuned.ipynb Cell 15\u001b[0m in \u001b[0;36mcompute_and_save_embeddings\u001b[0;34m(inp_path, out_path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shreyas/Desktop/Project/data/Stylianou_resnet18_finetuned.ipynb#X43sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m img \u001b[39m=\u001b[39m read_image(query_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shreyas/Desktop/Project/data/Stylianou_resnet18_finetuned.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m input_tensor \u001b[39m=\u001b[39m normalize(resize(img, (\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)) \u001b[39m/\u001b[39m \u001b[39m255.\u001b[39m, [\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], [\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shreyas/Desktop/Project/data/Stylianou_resnet18_finetuned.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out \u001b[39m=\u001b[39m finetuned_model(input_tensor\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shreyas/Desktop/Project/data/Stylianou_resnet18_finetuned.ipynb#X43sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m output_tensor \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shreyas/Desktop/Project/data/Stylianou_resnet18_finetuned.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m torch\u001b[39m.\u001b[39msave(output_tensor, out_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/query/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m file[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ets/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ets/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ets/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ets/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/ets/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "compute_and_save_embeddings(\"dataset/img_retrieval_CUB_200_2011\", \"embeddings_CUB_200_2011_ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, retrieve the saved embeddings, find the top-1 matched image using cosine similarity and save the heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_ops import load_and_resize, pil_bgr_to_rgb, combine_image_and_heatmap\n",
    "from similarity_ops import compute_spatial_similarity\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "AdaptiveAvgPool2d(output_size=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "for name, param in finetuned_model.named_children():\n",
    "    print(name)\n",
    "    if name == '8':\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "stylianou_model = nn.Sequential(*list(finetuned_model.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Sequential(\n",
      "  (0): BasicBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): BasicBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for name, param in stylianou_model.named_children():\n",
    "    print(name)\n",
    "    if name == '7':\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylianou(img1_path, img2_path, save_path): \n",
    "    '''\n",
    "        Separate definition for CUB dataset, with only query-heatmap overlay as output.\n",
    "    '''\n",
    "    img1_filename = img1_path.split(\"/\")[-1][:-4]\n",
    "    img1 = read_image(img1_path)\n",
    "    img2 = read_image(img2_path)\n",
    "\n",
    "    # Preprocess\n",
    "    img1_norm = normalize(resize(img1, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    img2_norm = normalize(resize(img2, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "    features1 = stylianou_model(img1_norm.unsqueeze(0))\n",
    "    features2 = stylianou_model(img2_norm.unsqueeze(0))\n",
    "\n",
    "    c, h, w = features1.squeeze(0).shape\n",
    "\n",
    "    # Compute the similarity heatmap\n",
    "    conv1 = features1.squeeze(0).permute(1, 2, 0).detach().numpy().reshape(h*w, c)\n",
    "    conv2 = features2.squeeze(0).permute(1, 2, 0).detach().numpy().reshape(h*w, c)\n",
    "    similarity = compute_spatial_similarity(conv1, conv2)\n",
    "\n",
    "    similarity1, _ = similarity\n",
    "\n",
    "    dummy_arr = np.zeros((224, 224, 3))\n",
    "\n",
    "    img1_out = combine_image_and_heatmap(dummy_arr, similarity1)  # overlay heatmap on image\n",
    "\n",
    "    overlay_img = img1_out[:, :, :3]\n",
    "    sim_path = save_path + \"/\" + \"{}.jpg\".format(img1_filename)\n",
    "    cv2.imwrite(sim_path, overlay_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve the query embeddings, compute the cosine similarity with all the gallery embeddings, return the top 1 results and save whether top-1 class matches or not\n",
    "def retrieve_visualize(img_path : str, emb_path : str, vis_path: str, csv_path: str):\n",
    "    # create output directories\n",
    "    os.makedirs(vis_path, exist_ok=True)\n",
    "\n",
    "    query_path = emb_path + \"/query\"\n",
    "    gallery_path = emb_path + \"/gallery\"\n",
    "\n",
    "    # retrieve and visualize query images\n",
    "    df = pd.read_csv(csv_path, sep='\\t', encoding='utf-8')\n",
    "    \n",
    "    query_files = os.listdir(query_path)\n",
    "    for query_file in query_files:\n",
    "        query_emb = torch.load(query_path + \"/\" + query_file)\n",
    "        gallery_dirnames = os.listdir(gallery_path)\n",
    "        max_sim = -1\n",
    "        max_file_path = \"\"\n",
    "        for dirname in gallery_dirnames:\n",
    "            file_names = os.listdir(gallery_path + \"/\" + dirname)\n",
    "            for file in file_names:\n",
    "                gallery_emb = torch.load(gallery_path + \"/\" + dirname + \"/\" + file)\n",
    "                sim = torch.cosine_similarity(query_emb, gallery_emb, dim=0)\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                    max_file_path = dirname + \"/\" + file\n",
    "\n",
    "        print(\"Query : {} | Top reference : {}\".format(query_file, max_file_path))\n",
    "\n",
    "        correct = 0\n",
    "\n",
    "        # save whether the retrieved image is of the correct class or not\n",
    "        try:\n",
    "            query_class = '_'.join(query_file[:-3].split('_')[:-2])\n",
    "            query_class_id = int(class_dict[query_class.lower()])\n",
    "            max_class = max_file_path.split('/')[0].split('.')[1]\n",
    "            max_class_id = int(class_dict[max_class.lower()])\n",
    "\n",
    "            \n",
    "            if query_class_id == max_class_id:\n",
    "                correct = 1\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        query_imgname = query_file[:-3] + \".jpg\"\n",
    "        df.loc[df['img_name'] == query_imgname, 'correct'] = correct\n",
    "        stylianou(img_path + \"/query/\" + query_file[:-3] + \".jpg\", img_path + \"/gallery/\" + max_file_path[:-3] + \".jpg\", vis_path)\n",
    "    \n",
    "    df.to_csv(csv_path, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"dataset/img_retrieval_CUB_200_2011_ft\"\n",
    "emb_path = \"embeddings_CUB_200_2011_ft\"\n",
    "vis_path = \"visualizations_CUB_200_2011_ft/heatmaps\"\n",
    "csv_path = \"dataset/CUB_200_2011/annotations.csv\"\n",
    "retrieve_visualize(img_path, emb_path, vis_path, csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
