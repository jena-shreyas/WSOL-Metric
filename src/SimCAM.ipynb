{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Uh6uSNl5x56"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly8-opW05v6V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from image_ops import load_and_resize, preprocess_im, pil_bgr_to_rgb, combine_image_and_heatmap, combine_horz\n",
        "from similarity_ops import compute_spatial_similarity\n",
        "from torchvision.io.image import read_image\n",
        "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuymlGNH5v6Y"
      },
      "outputs": [],
      "source": [
        "type1 = 'faces'\n",
        "type2 = 'hotels'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdfW62yO5v6Z",
        "outputId": "1e284d10-3324-4c27-ddca-dbfb9235cd8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shreyas/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/shreyas/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/home/shreyas/anaconda3/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = resnet18(pretrained=True).eval()\n",
        "\n",
        "# Get your input\n",
        "img1 = read_image(type1 + \"1.jpg\")\n",
        "img2 = read_image(type2 + \"2.jpg\")\n",
        "\n",
        "# Preprocess\n",
        "img1_norm = normalize(resize(img1, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "img2_norm = normalize(resize(img2, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rbLgnT8C6K7N"
      },
      "source": [
        "Now, define a Sequential model made up of all layers till the pretrained model's last `conv` layer. This gives the corresponding feature maps, which will be used for visualization purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0JXJ5mm5v6b"
      },
      "outputs": [],
      "source": [
        "f = torch.nn.Sequential(*list(model.children())[:-2])  \n",
        "features1 = f(img1_norm.unsqueeze(0))\n",
        "features2 = f(img2_norm.unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcmFPe3W5v6c"
      },
      "outputs": [],
      "source": [
        "c, h, w = features1.squeeze(0).shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wEskxxez6d44"
      },
      "source": [
        "`NOTE` : The `compute_similarity` function takes as input the last conv layer feature maps in the shape (h<sub>c</sub> * w<sub>c</sub>, n<sub>c</sub>) where\n",
        "\n",
        "\n",
        "\n",
        "*   h<sub>c</sub>, w<sub>c</sub> - Dimensions of each feature map\n",
        "*   n<sub>c</sub> - Number of feature maps.\n",
        "\n",
        "Hence, the feature maps must be flattened and reshaped in the proper format.\n",
        "\n",
        "*  `permute(1, 2, 0)` converts (h<sub>c</sub> , w<sub>c</sub>, n<sub>c</sub>) to (n<sub>c</sub>, h<sub>c</sub> * w<sub>c</sub>),\n",
        "*  `reshape` is then used to flatten matrix to (h<sub>c</sub> * w<sub>c</sub>, n<sub>c</sub>).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1HF_j2_5v6c"
      },
      "outputs": [],
      "source": [
        "# Compute the similarity heatmap\n",
        "conv1 = features1.squeeze(0).permute(1, 2, 0).detach().numpy().reshape(h*w, c)\n",
        "conv2 = features2.squeeze(0).permute(1, 2, 0).detach().numpy().reshape(h*w, c)\n",
        "similarity = compute_spatial_similarity(conv1, conv2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoYWGoVB5v6d"
      },
      "outputs": [],
      "source": [
        "similarity1, similarity2 = similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug0m1t725v6d",
        "outputId": "cd3fd96d-de1b-4468-ce2b-8c2355646fdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.00230161, 0.00375747, 0.00388819, 0.0057789 , 0.00650663,\n",
              "        0.00912857, 0.00840714],\n",
              "       [0.0036832 , 0.00643441, 0.00661743, 0.00961917, 0.01025735,\n",
              "        0.01443241, 0.01251629],\n",
              "       [0.00540263, 0.00750655, 0.00594269, 0.00773212, 0.00922846,\n",
              "        0.01459241, 0.01406103],\n",
              "       [0.00732416, 0.00944719, 0.00692369, 0.00806311, 0.00925338,\n",
              "        0.01417573, 0.01366571],\n",
              "       [0.00891512, 0.01091807, 0.00905613, 0.00905808, 0.0084312 ,\n",
              "        0.0113091 , 0.01077592],\n",
              "       [0.0099806 , 0.01516167, 0.02008093, 0.02404412, 0.02107863,\n",
              "        0.01740684, 0.0108393 ],\n",
              "       [0.00839548, 0.01214599, 0.01583638, 0.01941086, 0.01781347,\n",
              "        0.0133991 , 0.00804534]])"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similarity1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOrXfPp85v6e"
      },
      "outputs": [],
      "source": [
        "img1_path = type1 + \"1.jpg\"\n",
        "img2_path = type2 + \"2.jpg\"\n",
        "\n",
        "img1_arr = load_and_resize(img1_path)\n",
        "img2_arr = load_and_resize(img2_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1laECbKe8RCd"
      },
      "source": [
        "Above, we load the images as numpy array using `load_and_resize` (since `img1`, `img2` above are pytorch tensors - this loading as `np.array` is needed for the `combine_image_and_heatmap` function in the next cell.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCTOlrtS5v6e"
      },
      "outputs": [],
      "source": [
        "img1_out = combine_image_and_heatmap(img1_arr, similarity1)  # overlay heatmap on image\n",
        "img2_out = combine_image_and_heatmap(img2_arr, similarity2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-KFD-Qn5v6f"
      },
      "outputs": [],
      "source": [
        "sim_final = combine_horz([img1_out, img2_out])  # combine both overlayed images side by side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6AVD3wh5v6g"
      },
      "outputs": [],
      "source": [
        "sim_final_pil = Image.fromarray(np.uint8(sim_final))\n",
        "sim_bgr2rgb = pil_bgr_to_rgb(sim_final_pil)   # convert bgr image to rgb (final preprocessing needed ?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6xemnAy5v6g"
      },
      "outputs": [],
      "source": [
        "sim_path = \"sim_{}_{}.jpg\".format(type1, type2)\n",
        "sim_bgr2rgb.save(sim_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FGHyQ_w5v6h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
